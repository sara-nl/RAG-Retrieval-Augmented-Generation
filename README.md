# RAG-Retrieval-Augmented-Generation
This repository contains Retrieval-Augmented Generation


# Installation
Create virtual environment (one-time setup)
```bash
python -m venv venv
```
Activate virtual environment
```bash
source venv/bin/activate
```
Install necessary packages
```bash
pip install -r requirements.txt
```

## Large Language Model
The choice for the LLM can be any GGUF format (for CPU inference) or regular Huggingface LLMs
LLM used for CPU inference is: https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/tree/main
LLM used for GPU inference is: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha


For the embedding model, we used: https://huggingface.co/BAAI/bge-large-en

# Usage
- `app.py`: Activate interactive (UI/bash) script where queries can be given as input and a natural response given the vector database is generated by the Large Language Model
More information: `python app.py --help`
- `ingest.py`: Convert the custom dataset into a vector database by embedding the chunked documents. Only has to be run once if the data is static
More information: `python ingest.py --help`

# TODO:
- Enable GPU support âœ…
- Allow for data accumulation
